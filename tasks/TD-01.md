# TD-01 — Production Readiness и доработка A0-A2

## Статус выполнения (22.12.2024)

### Выполнено (P0 - критично для staging)
- ✅ **MongoDB Replica Set** - 3 ноды работают (PRIMARY + 2 SECONDARY)
  - Автоинициализация через `infra/mongo-init/03_replica_set.js`
  - Change Streams проверены и работают
  - URI обновлен в `.env.example` и `docker-compose.yml`

- ✅ **Health Checks с зависимостями** - `/health` эндпоинт расширен
  - Проверяет MongoDB (timeout 1s)
  - Проверяет Redis (timeout 500ms)
  - Возвращает HTTP 503 при недоступности зависимостей
  - Код: `backend/rust-api/src/handlers/mod.rs`

### Выполнено (P1 - для production)
- ✅ **Мониторинг инфраструктура** - стек развернут с профилем `monitoring`
  - Prometheus (порт 9090) - собирает метрики
  - Jaeger (порт 16686) - готов для трейсинга
  - MongoDB Exporter (порт 9216) - экспортирует метрики
  - Redis Exporter (порт 9121) - экспортирует метрики
  - Grafana (порт 3000) - дашборды можно настроить
  - Конфиги: `infra/prometheus/`, `infra/grafana/`

### Выполнено (P1 - для production) - метрики
- ✅ **Prometheus метрики в Rust API** - добавлен `/metrics` endpoint с prometheus crate
  - Добавлены зависимости: prometheus 0.14, lazy_static 1.4
  - Создан модуль `src/metrics.rs` с 13 типами метрик (HTTP, DB, Cache, Business)
  - Реализован middleware для автоматического сбора HTTP метрик
  - Эндпоинт `/metrics` возвращает метрики в формате Prometheus
  - Сервисы проинструментированы: session_service, answer_service
  - Prometheus активно скрапит метрики (таргет rust-api:8081 UP)

### В работе / TODO
- ⏳ **Grafana дашборды** - создать JSON дашборды для API/MongoDB/Redis
- ❌ **Backup/restore тестирование** - не выполнено
- ❌ **Нагрузочное тестирование** - k6 сценарии есть, нужна валидация SLA
- ❌ **JWT ротация** - скрипт не создан
- ❌ **MongoDB Field Level Encryption** - не настроено
- ❌ **TLS** - не включен (только для dev)
- ❌ **Alerting** - не настроен
- ❌ **Runbooks** - не созданы

## Контекст
После выполнения задач A0, A1 и A2 проведена проверка соответствия реализации требованиям. Выявлено несколько технических долгов и областей для улучшения, критичных для production deployment. Данная задача устраняет эти недоработки и обеспечивает полную готовность инфраструктуры к развертыванию в staging/production окружениях.

## Выявленные проблемы

### Критичные
1. **MongoDB Change Streams отключены в dev** — требуется replica set для работы Change Stream Bridge (`infra/scripts/changestream_bridge.py`), что блокирует тестирование потока MongoDB → Redis → Embedding Pipeline.
2. **Backup/restore не протестированы** — скрипты `infra/scripts/backup.sh` и `restore.sh` созданы, но не проверены на реальных данных. Нет гарантии восстановления ≤15 минут (SLA requirement из A1).
3. **Отсутствует нагрузочное тестирование** — не подтверждён SLA ≤200 мс p95 для Rust API при 500 rps (требование из A2).

### Важные
4. **JWT секреты хардкодированы** — `JWT_SECRET=your-secret-key-change-in-prod` в `.env.example`, нет механизма ротации.
5. **MongoDB Field Level Encryption не настроено** — PII данные (email, sso_id) не шифруются на уровне БД (требование A1, раздел 1).
6. **TLS не включен** — все соединения MongoDB/Redis/Qdrant работают по HTTP без шифрования.
7. **Monitoring не подключен** — OpenTelemetry настроено в коде, но нет Jaeger/Tempo для трейсов и дашбордов.

### Желательные
8. **Отсутствуют алерты** — incidents записываются в MongoDB/Redis Pub/Sub, но нет интеграции с Alertmanager для уведомлений DevOps.
9. **Документация по troubleshooting** — нет runbook для типичных проблем (Redis OOM, MongoDB replica set failover, Qdrant crash recovery).
10. **Health checks не полноценные** — `/health` в Rust API проверяет только процесс, но не состояние зависимостей (MongoDB, Redis).

## Состав работ

### 1. MongoDB Replica Set для dev окружения ✅ ВЫПОЛНЕНО
- ✅ Обновить `docker-compose.yml`: создать 3 MongoDB ноды (primary + 2 secondary) с общим keyfile.
  - Реализовано с keyfile внутри контейнера (Windows-совместимо)
  - Порты: 27017 (primary), 27018 (secondary1), 27019 (secondary2)
- ✅ Автоматизировать инициализацию replica set через `rs.initiate()` в `infra/mongo-init/03_replica_set.js`.
  - Скрипт создан, запускается через `docker-compose --profile init run mongodb-init`
  - Проверяет статус, выбирает PRIMARY, ждет завершения
- ✅ Обновить `MONGODB_URI` в `.env.example` для подключения к replica set (`?replicaSet=rs0`).
  - URI: `mongodb://admin:changeme@localhost:27017,localhost:27018,localhost:27019/trainingground?authSource=admin&replicaSet=rs0`
- ✅ Протестировать Change Stream Bridge:
  - ✅ Change Streams работают (протестировано через mongosh)
  - ⚠️ `changestream_bridge.py` работает, но требует запуска внутри Docker network (из хоста не резолвятся имена контейнеров)
  - ❌ `docs/mongodb-replica-set.md` не создан

### 2. Тестирование Backup/Restore ❌ НЕ ВЫПОЛНЕНО
- ❌ Создать тестовый датасет (seed data):
  - 100 users, 10 groups, 50 templates, 1000 tasks, 500 attempts.
  - 3 Qdrant коллекции с векторами (по 100 точек каждая).
  - Redis ключи: 50 sessions, 20 hints_used, 10 anticheat counters.
- ❌ Выполнить полный backup через `infra/scripts/backup.sh`:
  - Проверить создание архивов в `/tmp/backups_*`.
  - Проверить upload в Yandex Object Storage (mock или реальный S3).
- ❌ Выполнить restore через `infra/scripts/restore.sh`:
  - Очистить локальные БД (`docker-compose down -v`).
  - Восстановить из backup.
  - Проверить консистентность данных (count коллекций, checksum).
  - Измерить время восстановления (должно быть ≤15 минут).
- ❌ Задокументировать результаты в `docs/backup-testing.md`.
**Примечание**: Скрипты backup.sh и restore.sh существуют в `infra/scripts/`, но не протестированы.

### 3. Нагрузочное тестирование Rust API ⏳ ЧАСТИЧНО ВЫПОЛНЕНО
- ✅ Создать k6/vegeta сценарии в `tests/load/`:
  - ✅ **Scenario 1**: 500 rps на `POST /sessions/{id}/answers` (5 минут sustained) - существует `tests/performance/answers.js`
  - ✅ **Scenario 2**: 50 rps на `POST /sessions/{id}/hints` (3 минуты) - существует `tests/performance/hints.js`
  - ✅ **Scenario 3**: 10 параллельных SSE потоков `GET /sessions/{id}/stream` - существует `tests/performance/sse.js`
  - ❌ **Scenario 4**: Mixed workload (70% answers, 20% sessions, 10% hints) - не создан
- ⏳ Запустить в локальном Docker окружении с realistic delays (MongoDB 5ms, Redis 1ms).
  - Ранее запускались тесты, результаты в `tests/performance/results_*.json`
  - Нужно повторить с новым replica set
- ⏳ Собрать метрики:
  - p50, p95, p99 latency для каждого эндпоинта.
  - Throughput (rps).
  - Error rate (должен быть <0.1%).
- ❌ Проверить SLA: p95 ≤200 мс для `POST /answers` (требование A2) - нужна валидация.
- ❌ Создать отчёт `docs/load-testing.md` с графиками и выводами.

### 4. Безопасность: JWT ротация и секреты ❌ НЕ ВЫПОЛНЕНО
- ❌ Создать скрипт `infra/scripts/rotate_jwt_secret.sh`:
  - Генерация нового RSA ключа (2048 бит).
  - Обновление `JWT_SECRET` в `.env` и restart сервисов.
  - Graceful migration: поддержка 2 ключей (старый + новый) на 24 часа.
- ❌ Интегрировать с Hashicorp Vault (опционально) или Yandex Lockbox:
  - Хранение секретов (JWT_SECRET, REDIS_PASSWORD, QDRANT_API_KEY).
  - Автоматическая ротация раз в 90 дней.
- ❌ Обновить `backend/rust-api/src/config.rs` для чтения секретов из Vault.
- ❌ Задокументировать процесс в `docs/security/jwt-rotation.md`.
**Примечание**: Существует `infra/scripts/generate_secrets.sh` для генерации паролей, но не для ротации.

### 5. MongoDB Field Level Encryption ❌ НЕ ВЫПОЛНЕНО
- ❌ Настроить Client-Side Field Level Encryption для коллекции `users`:
  - Поля `email`, `sso_id` должны шифроваться с помощью AES-256.
  - Ключ шифрования хранится в Yandex KMS (Key Management Service).
- ❌ Обновить `infra/mongo-init/01_init_db.js`:
  - Добавить schema с `encrypt` для PII полей.
  - Протестировать, что данные не читаемы без ключа.
- ❌ Создать инструкцию `docs/security/field-level-encryption.md`.

### 6. TLS для всех соединений ❌ НЕ ВЫПОЛНЕНО
- ❌ **MongoDB**:
  - Сгенерировать самоподписанный сертификат для dev (`infra/certs/mongodb.pem`).
  - Обновить `docker-compose.yml`: добавить `--tlsMode requireTLS`.
  - Обновить `MONGODB_URI`: `?tls=true&tlsCAFile=/certs/ca.pem`.
- ❌ **Redis**:
  - Настроить `redis.conf` с `tls-port 6380` и сертификатами.
  - Обновить Rust API Redis client для TLS соединений.
- ❌ **Qdrant**:
  - Настроить HTTPS для API через Nginx reverse proxy.
  - Обновить `QDRANT_URL=https://localhost:6333`.
- ❌ Для production: использовать Let's Encrypt / Yandex Certificate Manager.
- ❌ Задокументировать в `docs/security/tls-setup.md`.
**Примечание**: Для dev окружения TLS не критично, для production обязательно.

### 7. Мониторинг и Observability ⏳ ЧАСТИЧНО ВЫПОЛНЕНО

#### 7.1. Развертывание стека мониторинга ✅ ВЫПОЛНЕНО

**Обновить `docker-compose.yml` (добавить profile `monitoring`):**

```yaml
# docker-compose.yml (добавить в конец файла)

  # Prometheus для сбора метрик
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: trainingground-prometheus
    profiles: ["monitoring"]
    ports:
      - "9090:9090"
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./infra/prometheus/alerts.yml:/etc/prometheus/alerts.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - trainingground-network

  # Grafana для визуализации
  grafana:
    image: grafana/grafana:10.2.2
    container_name: trainingground-grafana
    profiles: ["monitoring"]
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=redis-datasource
    volumes:
      - ./infra/grafana/provisioning:/etc/grafana/provisioning
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    networks:
      - trainingground-network
    depends_on:
      - prometheus

  # Jaeger для distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: trainingground-jaeger
    profiles: ["monitoring"]
    ports:
      - "16686:16686"  # Jaeger UI
      - "4317:4317"    # OTLP gRPC receiver
      - "4318:4318"    # OTLP HTTP receiver
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - trainingground-network

  # MongoDB Exporter
  mongodb-exporter:
    image: percona/mongodb_exporter:0.40
    container_name: trainingground-mongodb-exporter
    profiles: ["monitoring"]
    ports:
      - "9216:9216"
    environment:
      - MONGODB_URI=mongodb://${MONGO_USER:-admin}:${MONGO_PASSWORD:-password}@mongodb:27017
    networks:
      - trainingground-network
    depends_on:
      - mongodb

  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:v1.55.0
    container_name: trainingground-redis-exporter
    profiles: ["monitoring"]
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redispass}
    networks:
      - trainingground-network
    depends_on:
      - redis

volumes:
  prometheus_data:
  grafana_data:
```

**Создать `infra/prometheus/prometheus.yml`:**

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# Загрузка правил алертинга
rule_files:
  - 'alerts.yml'

# Настройка scrape targets
scrape_configs:
  # Rust API метрики
  - job_name: 'rust-api'
    static_configs:
      - targets: ['rust-api:8081']
    metrics_path: '/metrics'

  # MongoDB метрики
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb-exporter:9216']

  # Redis метрики
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

#### 7.2. Настройка метрик в Rust API ✅ ВЫПОЛНЕНО

**Реализация (22.12.2024):**
- Добавлены зависимости в `backend/rust-api/Cargo.toml`: prometheus 0.14, lazy_static 1.4
- Создан модуль `src/metrics.rs` с 13 типами метрик:
  - HTTP_REQUESTS_TOTAL, HTTP_REQUEST_DURATION_SECONDS (latency buckets)
  - DB_OPERATIONS_TOTAL, DB_OPERATION_DURATION_SECONDS (MongoDB)
  - CACHE_OPERATIONS_TOTAL, CACHE_HIT_RATIO, CACHE_OPERATION_DURATION_SECONDS (Redis)
  - SESSIONS_TOTAL, SESSIONS_ACTIVE, ANSWERS_SUBMITTED_TOTAL, HINTS_REQUESTED_TOTAL (Business)
  - SSE_CONNECTIONS_ACTIVE, ANTICHEAT_VIOLATIONS_TOTAL
- Создан middleware `src/middlewares/metrics.rs` для автоматического сбора HTTP метрик с нормализацией путей
- Добавлен эндпоинт `GET /metrics` в `src/handlers/mod.rs`
- Проинструментированы сервисы: session_service.rs, answer_service.rs
- Prometheus успешно скрапит метрики с rust-api:8081 (таргет UP)

**Добавленные зависимости в `backend/rust-api/Cargo.toml`:**

```toml
[dependencies]
# Metrics
prometheus = "0.14"
lazy_static = "1.4"
```

**Создать `backend/rust-api/src/metrics.rs`:**

```rust
use lazy_static::lazy_static;
use prometheus::{
    register_histogram_vec, register_int_counter_vec, register_int_gauge_vec,
    HistogramVec, IntCounterVec, IntGaugeVec, TextEncoder, Encoder,
};

lazy_static! {
    // HTTP request duration (для latency p50/p95/p99)
    pub static ref HTTP_REQUEST_DURATION: HistogramVec = register_histogram_vec!(
        "http_request_duration_seconds",
        "HTTP request latency in seconds",
        &["method", "endpoint", "status"],
        vec![0.005, 0.01, 0.025, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
    ).unwrap();

    // HTTP request counter (для throughput и error rate)
    pub static ref HTTP_REQUESTS_TOTAL: IntCounterVec = register_int_counter_vec!(
        "http_requests_total",
        "Total number of HTTP requests",
        &["method", "endpoint", "status"]
    ).unwrap();

    // Session метрики
    pub static ref SESSIONS_ACTIVE: IntGaugeVec = register_int_gauge_vec!(
        "sessions_active",
        "Number of active sessions",
        &["user_id"]
    ).unwrap();

    // Hint метрики
    pub static ref HINTS_REQUESTS_TOTAL: IntCounterVec = register_int_counter_vec!(
        "hints_requests_total",
        "Total hint requests",
        &["source"]  // cache, python_api, fallback
    ).unwrap();

    pub static ref HINTS_CACHE_HIT_RATE: IntCounterVec = register_int_counter_vec!(
        "hints_cache_operations_total",
        "Cache operations (hits/misses)",
        &["operation"]  // hit, miss
    ).unwrap();

    // Anticheat метрики
    pub static ref ANTICHEAT_INCIDENTS_TOTAL: IntCounterVec = register_int_counter_vec!(
        "anticheat_incidents_total",
        "Total anticheat incidents",
        &["severity", "action"]  // severity: low/medium/high/critical, action: flagged/blocked
    ).unwrap();

    pub static ref ANTICHEAT_USERS_BLOCKED: IntGaugeVec = register_int_gauge_vec!(
        "anticheat_users_blocked",
        "Number of currently blocked users",
        &[]
    ).unwrap();

    // Database метрики
    pub static ref DB_QUERY_DURATION: HistogramVec = register_histogram_vec!(
        "db_query_duration_seconds",
        "Database query duration",
        &["database", "operation"],  // database: mongodb/redis, operation: get/set/insert
        vec![0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
    ).unwrap();
}

// Эндпоинт /metrics для Prometheus
pub async fn metrics_handler() -> String {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = Vec::new();
    encoder.encode(&metric_families, &mut buffer).unwrap();
    String::from_utf8(buffer).unwrap()
}
```

**Интегрировать метрики в хэндлеры (пример для `answer_service.rs`):**

```rust
use crate::metrics::{HTTP_REQUEST_DURATION, HTTP_REQUESTS_TOTAL, DB_QUERY_DURATION};
use std::time::Instant;

pub async fn submit_answer(...) -> Result<SubmitAnswerResponse> {
    let start = Instant::now();

    // Бизнес-логика...
    let db_start = Instant::now();
    let task = self.get_task(task_id).await?;
    DB_QUERY_DURATION
        .with_label_values(&["mongodb", "get_task"])
        .observe(db_start.elapsed().as_secs_f64());

    // После завершения запроса
    let duration = start.elapsed().as_secs_f64();
    HTTP_REQUEST_DURATION
        .with_label_values(&["POST", "/sessions/{id}/answers", "200"])
        .observe(duration);
    HTTP_REQUESTS_TOTAL
        .with_label_values(&["POST", "/sessions/{id}/answers", "200"])
        .inc();

    Ok(response)
}
```

**Добавить эндпоинт `/metrics` в `main.rs`:**

```rust
use axum::routing::get;
mod metrics;

let app = Router::new()
    // ... существующие роуты
    .route("/metrics", get(metrics::metrics_handler));
```

#### 7.3. Настройка OpenTelemetry для Jaeger ⏳ ЧАСТИЧНО ВЫПОЛНЕНО
**Примечание**: Jaeger запущен (docker-compose), OpenTelemetry уже настроен в коде, нужно добавить spans в бизнес-логику.

**Обновить `backend/rust-api/src/main.rs` (уже частично настроено):**

```rust
use opentelemetry::global;
use opentelemetry_sdk::trace::TracerProvider;
use opentelemetry_otlp::WithExportConfig;
use tracing_subscriber::layer::SubscriberExt;

#[tokio::main]
async fn main() {
    // Настройка OTLP exporter для Jaeger
    let otlp_endpoint = std::env::var("OTEL_EXPORTER_OTLP_ENDPOINT")
        .unwrap_or_else(|_| "http://jaeger:4318".to_string());

    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .http()
                .with_endpoint(otlp_endpoint)
        )
        .with_trace_config(
            opentelemetry_sdk::trace::config()
                .with_resource(opentelemetry_sdk::Resource::new(vec![
                    opentelemetry::KeyValue::new("service.name", "rust-api"),
                ]))
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)
        .expect("Failed to install OpenTelemetry tracer");

    let telemetry = tracing_opentelemetry::layer().with_tracer(tracer);

    // Subscriber с OTLP
    let subscriber = tracing_subscriber::registry()
        .with(telemetry)
        .with(tracing_subscriber::fmt::layer());

    tracing::subscriber::set_global_default(subscriber)
        .expect("Failed to set subscriber");

    // ... остальной код запуска сервера
}
```

**Добавить spans в критичные функции:**

```rust
use tracing::{instrument, info_span};

#[instrument(skip(self), fields(session_id, user_id))]
pub async fn submit_answer(&self, session_id: &str, user_id: &str, ...) -> Result<...> {
    let span = info_span!("check_anticheat", user_id = %user_id);
    let _guard = span.enter();

    // Логика...

    Ok(response)
}
```

#### 7.4. Создание Grafana дашбордов

**Структура файлов:**
```
infra/grafana/
├── provisioning/
│   ├── datasources/
│   │   └── prometheus.yml
│   └── dashboards/
│       └── dashboards.yml
└── dashboards/
    ├── api-performance.json
    ├── hints-service.json
    ├── anticheat.json
    └── database.json
```

**`infra/grafana/provisioning/datasources/prometheus.yml`:**

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
```

**`infra/grafana/provisioning/dashboards/dashboards.yml`:**

```yaml
apiVersion: 1

providers:
  - name: 'TrainingGround'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

**Примеры панелей для `api-performance.json` (PromQL запросы):**

1. **Latency p50/p95/p99:**
```promql
# p50
histogram_quantile(0.50,
  rate(http_request_duration_seconds_bucket{endpoint="/sessions/{id}/answers"}[5m])
)

# p95
histogram_quantile(0.95,
  rate(http_request_duration_seconds_bucket{endpoint="/sessions/{id}/answers"}[5m])
)

# p99
histogram_quantile(0.99,
  rate(http_request_duration_seconds_bucket{endpoint="/sessions/{id}/answers"}[5m])
)
```

2. **Throughput (requests per second):**
```promql
sum(rate(http_requests_total{endpoint="/sessions/{id}/answers"}[1m])) by (endpoint)
```

3. **Error Rate (%):**
```promql
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m])) * 100
```

4. **Cache Hit Rate для подсказок:**
```promql
sum(rate(hints_cache_operations_total{operation="hit"}[5m]))
/
sum(rate(hints_cache_operations_total[5m])) * 100
```

5. **Anticheat Incidents (last hour):**
```promql
sum(increase(anticheat_incidents_total[1h])) by (severity)
```

6. **MongoDB Query Duration p95:**
```promql
histogram_quantile(0.95,
  rate(db_query_duration_seconds_bucket{database="mongodb"}[5m])
)
```

7. **Redis Memory Usage:**
```promql
redis_memory_used_bytes / redis_memory_max_bytes * 100
```

#### 7.5. Работа с инструментами

**Запуск мониторинга:**
```bash
# Запустить все сервисы + мониторинг
docker-compose --profile monitoring up -d

# Проверить статус
docker-compose --profile monitoring ps
```

**Доступ к UI:**
- **Grafana**: http://localhost:3000 (admin / admin)
- **Prometheus**: http://localhost:9090
- **Jaeger**: http://localhost:16686

**Типичные задачи:**

1. **Проверить текущий latency API:**
   - Открыть Grafana → Dashboard "API Performance"
   - Посмотреть график p95 за последние 15 минут
   - Если >200 мс — искать медленные запросы в Jaeger

2. **Найти медленный запрос в Jaeger:**
   - Открыть Jaeger UI → Service: "rust-api"
   - Filters: Min Duration = 500ms
   - Find Traces → выбрать trace с большой длительностью
   - Изучить spans: где именно задержка (MongoDB query? Redis?)

3. **Анализ cache hit rate:**
   - Grafana → "Hints Service" dashboard
   - Если hit rate <80% → проверить TTL кэша (может быть слишком короткий)

4. **Расследование anticheat инцидента:**
   - Grafana → "Anticheat" dashboard
   - Spike в incidents? → Посмотреть Redis ключ `anticheat:{user_id}`
   - Проверить логи: `docker-compose logs rust-api | grep "user_id=..."`

#### 7.6. Документация

**Создать `docs/monitoring/setup.md`:**
- Инструкции по развертыванию стека
- Описание каждого дашборда и панелей
- Примеры PromQL запросов
- Troubleshooting (Prometheus не scrape метрики, Grafana не видит datasource)

**Создать `docs/monitoring/dashboards.md`:**
- Скриншоты дашбордов
- Объяснение каждой метрики
- Пороговые значения для алертов
- Примеры интерпретации графиков

**Создать `docs/monitoring/jaeger-guide.md`:**
- Как искать медленные запросы
- Анализ distributed traces
- Примеры типичных проблем (N+1 queries, медленный Redis)
- Integration с логами (correlation ID)

### 8. Alerting через Alertmanager ❌ НЕ ВЫПОЛНЕНО
- ❌ Развернуть Alertmanager в `docker-compose.yml`.
- ❌ Создать правила алертинга (`infra/prometheus/alerts.yml`):
  - **Critical**: API p95 latency >500 мс (5 минут sustained).
  - **Critical**: Error rate >1% (3 минуты sustained).
  - **Warning**: Redis memory >80% capacity.
  - **Warning**: MongoDB replica set member down.
  - **Warning**: Anticheat incidents >10/час.
  - **Info**: Backup failed или duration >15 минут.
- Интегрировать с Telegram/Slack/Email для уведомлений.
- Создать runbook `docs/runbooks/incident-response.md`.

### 9. Health Checks с зависимостями ✅ ВЫПОЛНЕНО
- ✅ Обновить `GET /health` в Rust API:
  - ✅ Проверка MongoDB: `db.runCommand("ping")` с timeout 1s.
  - ✅ Проверка Redis: `PING` с timeout 500ms.
  - ❌ Проверка Qdrant: не добавлена (можно добавить позже)
  - ✅ Возвращать:
    - `200 OK` — все зависимости доступны.
    - `503 Service Unavailable` — хотя бы одна зависимость недоступна (с деталями в JSON).
  - Код: `backend/rust-api/src/handlers/mod.rs`
- ❌ Добавить `GET /ready` для Kubernetes readiness probe - не добавлен.
- ❌ Задокументировать формат ответа в `docs/api/health.yaml` - не создано.

### 10. Troubleshooting Runbooks ❌ НЕ ВЫПОЛНЕНО
- ❌ Создать серию runbook'ов в `docs/runbooks/`:
  - **Redis OOM**: действия при исчерпании памяти (eviction policy, увеличение RAM).
  - **MongoDB Replica Set Failover**: процедура восстановления primary ноды.
  - **Qdrant Index Corruption**: rebuild индекса из snapshot.
  - **Anticheat False Positives**: ручная разблокировка пользователя.
  - **Change Stream Lag**: диагностика и очистка очереди `content:changes`.
- Каждый runbook включает:
  - Симптомы (как обнаружить проблему).
  - Диагностика (команды для проверки).
  - Решение (пошаговые действия).
  - Escalation (когда привлекать senior engineer).

## Ожидаемые артефакты

### Инфраструктура
- `docker-compose.yml` с MongoDB replica set, Jaeger, Prometheus, Grafana, Alertmanager.
- Скрипты: `rotate_jwt_secret.sh`, обновлённые `backup.sh`/`restore.sh`.
- Сертификаты TLS (self-signed для dev, инструкции для production).

### Тестирование
- Seed data скрипт `infra/scripts/seed_data.py`.
- k6/vegeta сценарии в `tests/load/`.
- Отчёты: `docs/backup-testing.md`, `docs/load-testing.md`.

### Безопасность
- Vault/KMS интеграция (или инструкции по настройке).
- Field Level Encryption для MongoDB.
- TLS конфигурация для всех сервисов.

### Мониторинг
- Grafana дашборды (JSON).
- Prometheus alerts.
- Alertmanager интеграция с Telegram/Slack.

### Документация
- `docs/security/`: jwt-rotation.md, field-level-encryption.md, tls-setup.md.
- `docs/monitoring/`: setup.md, dashboards.md.
- `docs/runbooks/`: 5+ runbook файлов.
- `docs/backup-testing.md`, `docs/load-testing.md`.
- Обновлённый `docs/dev-setup.md` с инструкциями для replica set.

## Чек-лист готовности (22.12.2024)

### Критичные (P0)
- [x] **MongoDB replica set работает в dev окружении** ✅
  - 3 ноды: PRIMARY + 2 SECONDARY
  - Change Streams функционируют (протестировано через mongosh)
  - ⚠️ Change Stream Bridge требует запуска внутри Docker network
- [ ] **Backup/restore проверен** ❌ - скрипты есть, но не протестированы
- [ ] **Нагрузочное тестирование выполнено** ⏳ - k6 сценарии есть, нужна валидация SLA

### Важные (P1)
- [ ] **JWT ротация** ❌ - скрипт не создан
- [ ] **MongoDB Field Level Encryption** ❌ - не настроено
- [ ] **TLS** ❌ - не включен (только для dev)
- [x] **Jaeger/Grafana развёрнуты** ✅ - запущены с профилем `monitoring`
  - ⏳ Дашборды: конфиги созданы, JSON дашборды не созданы
  - ✅ Метрики: exporters работают, /metrics в Rust API добавлен и работает

### Желательные (P2)
- [ ] **Alertmanager** ❌ - не настроен
- [x] **Health checks проверяют зависимости** ✅ - MongoDB/Redis проверяются
- [ ] **Runbooks** ❌ - не созданы

## Тестирование

### MongoDB Replica Set
- **Failover test**: остановить primary ноду, проверить автоматическое переключение на secondary (≤30 сек).
- **Change Stream test**: создать 100 template updates, проверить все события попали в Redis Stream.
- **Recovery test**: удалить 1 ноду, добавить новую, проверить синхронизацию данных.

### Backup/Restore
- **Happy path**: backup → restore → verify checksums.
- **Partial failure**: удалить MongoDB backup файл, проверить fallback на предыдущую версию.
- **Performance**: измерить time to recovery при 10GB данных (должен быть ≤15 мин).

### Нагрузочное тестирование
- **Baseline**: запустить без load, измерить latency (baseline p95 ~20-50 мс).
- **Target load**: 500 rps на `/answers`, проверить p95 ≤200 мс.
- **Overload**: 1000 rps, проверить graceful degradation (rate limiting, не крашится).
- **Soak test**: 100 rps в течение 1 часа, проверить отсутствие memory leaks.

### Безопасность
- **JWT rotation**: ротировать ключ, проверить, что старые токены работают 24 часа, затем отклоняются.
- **Encryption**: записать user с email, проверить, что в mongodump данные зашифрованы.
- **TLS**: проверить, что подключение без TLS отклоняется (`requireTLS` mode).

### Мониторинг
- **Metrics test**: отправить 100 requests, проверить появление метрик в Prometheus.
- **Alert test**: создать искусственный spike latency (sleep в коде), проверить срабатывание алерта.
- **Dashboard test**: проверить, что все панели Grafana отображают данные.

### Health Checks
- **All healthy**: проверить `GET /health` возвращает 200.
- **MongoDB down**: остановить MongoDB, проверить `GET /health` возвращает 503 с деталями.
- **Redis down**: остановить Redis, проверить 503.
- **Partial failure**: остановить Qdrant (если опционально), проверить, что API продолжает работать.

## Приоритизация

### P0 (Must-have для staging)
1. MongoDB replica set
2. Backup/restore тестирование
3. TLS для всех соединений
4. Health checks с зависимостями

### P1 (Must-have для production)
5. Нагрузочное тестирование + SLA validation
6. JWT секреты в Vault/KMS
7. Мониторинг (Jaeger + Grafana)
8. Alerting

### P2 (Nice-to-have)
9. MongoDB Field Level Encryption
10. Runbooks

## Зависимости
- **Блокирует**: A3 (Python Explanation API) требует работающий Change Stream Bridge для Embedding Pipeline.
- **Требует**: Завершение A0, A1, A2 (выполнено).
- **Опционально**: интеграция с Yandex Cloud (Lockbox для секретов, Certificate Manager для TLS).

## Оценка времени
- **MongoDB replica set**: 8 часов (настройка + тестирование failover).
- **Backup/restore тестирование**: 6 часов (seed data + automation + validation).
- **Нагрузочное тестирование**: 12 часов (сценарии k6 + анализ результатов + оптимизация).
- **Безопасность (JWT + TLS + Encryption)**: 16 часов (сложная настройка + интеграция с KMS).
- **Мониторинг + Alerting**: 10 часов (дашборды + алерты + runbooks).
- **Документация**: 6 часов (runbooks + troubleshooting guides).

**Итого**: ~58 часов (≈7-8 рабочих дней для 1 разработчика, ≈3-4 дня для 2 разработчиков параллельно).

## Успешное завершение
Задача считается выполненной, когда:
1. MongoDB replica set работает в dev/staging, Change Streams функционируют.
2. Backup/restore протестирован, восстановление ≤15 минут документально подтверждено.
3. Нагрузочное тестирование показало SLA ≤200 мс p95 при 500 rps.
4. Все секреты (JWT, passwords, API keys) хранятся в Vault/KMS, не в `.env`.
5. TLS включен для MongoDB, Redis, Qdrant (self-signed в dev, валидные сертификаты в production).
6. Jaeger + Grafana показывают трейсы и метрики, алерты приходят в Telegram/Slack.
7. Health checks корректно определяют состояние зависимостей.
8. Runbooks созданы для 5+ типичных проблем.

После выполнения TD-01 проект будет **production-ready** и можно переходить к развёртыванию в staging/production окружениях.
